{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Failure of the Pump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Problem Identification\n",
    "\n",
    "#### Problem Statement: \n",
    "    Detect anomalies in the sensor readings from the pump sensors and predict the next failure of the pump with at least \n",
    "    75% accuracy. \n",
    "        \n",
    "#### Context:\n",
    "\n",
    "    Company XYZ operates a water distribution plant that supplies water to the town of ABC. The water distribution system \n",
    "    consists of number of pumps and one of the main pumps failed 7 times last year which resulted in an interruption of \n",
    "    water supplly for the households in the town and lead to some serious health problems for number of families. The team \n",
    "    of reliability engineers could not see any pattern in the data when the system failed and therefore could not identify \n",
    "    the cause of the problem. In order to prevent the same failure from happening in the future again, the XYZ company \n",
    "    wants to be able to detect the potential issue in advance and be able to control and mitigate the risk of failure. \n",
    "    There is one particular pump that is the most important and is equiped with 51 sensors which measure different types of \n",
    "    functionalities of the pump. \n",
    "\n",
    "#### Criteria For Success:\n",
    "    A model that predicts with at least 75% accuracy and generalizes on other samples without underfitting or overfitting.\n",
    "\n",
    "#### Scope of the Solution Space:\n",
    "    The scope of this project is limited to developing a model that detects anomalies and predicts failures of water pumps \n",
    "    based on raw sensor data. The model will not be intended to be used for detecting credit frauds or any other use cases \n",
    "    outside the scope of this project. \n",
    "\n",
    "#### Contraints:\n",
    "    Data set is limited to the sensor readings from a single pump hence may not be the best representation of all the \n",
    "    pumps. Computing power might become a constraint for effectively visualizing all 51 features at the same time.\n",
    "\n",
    "#### Stakeholders:\n",
    "    Samwell Tarly - Maintenance Manager of the XYZ Company\n",
    "\n",
    "#### Data:\n",
    "    The data set is sourced from https://www.kaggle.com/nphantawee/pump-sensor-data and consists of 51 numerical features \n",
    "    and a categorical label. 51 numerical features contain raw sensor readings from 51 different sensors that are used to \n",
    "    do condition monitoring of the pump.The label contains string values that represent normal, broken and recovering \n",
    "    operational conditions of the pump. The data set represents 219,521 readings from 51 sensors.   \n",
    "\n",
    "#### Solution Approach: \n",
    "\n",
    "    I will solve this problem by developing a classification model that detects anomalies from the sensor readings and \n",
    "    accordingly predicts the potential failure of the pump. To do that, I will first apply an appropriate unsupervised \n",
    "    learning techniques to undertake dimensionality reduction for the effective visualization of the data and EDA. Then I \n",
    "    will train classification models and will use cross-validation to evaluate their performance to select the best model.\n",
    "\n",
    "#### Project Deliverables: \n",
    "    The deliverables include an app that takes certain features in certain format from a pump and predicts next failure, as \n",
    "    well as a slide deck that explains how the app was developed and verified. In addition, all the code will be available \n",
    "    in a Jupyter notebook in a GitHub repo for each step of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Pre-processing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory to where the data resides\n",
    "path = 'C:\\\\Users\\\\Bauyrjan.Jyenis\\\\Capstone2_Project\\\\capstone\\\\data\\\\processed'\n",
    "os.chdir(path)\n",
    "# import the tidy data from the Step #2\n",
    "df=pd.read_csv('step2-output.csv')\n",
    "# Delete the unnecessary column\n",
    "del df['Unnamed: 0']\n",
    "# Let's convert the data type of timestamp column to datatime format\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2: Dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORMAL        205067\n",
       "RECOVERING     14447\n",
       "BROKEN             7\n",
       "Name: machine_status, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['machine_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will combine RECOVERING AND BROKEN states under a single class of 0 that implies NOT NORMAL situation. And I will label NORMAL state as 1 that implies NORMAL.\n",
    "\n",
    "0: NOT NORMAL\n",
    "\n",
    "1: NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    205067\n",
       "0     14454\n",
       "Name: machine_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.copy()\n",
    "df2.loc[df2['machine_status'] == 'RECOVERING', 'machine_status'] = 0\n",
    "df2.loc[df2['machine_status'] == 'BROKEN', 'machine_status'] = 0\n",
    "df2.loc[df2['machine_status'] == 'NORMAL', 'machine_status'] = 1\n",
    "df2['machine_status'] = pd.to_numeric(df2['machine_status'])\n",
    "df2['machine_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3:  Classes are imbalanced  - Bootstrapping\n",
    "\n",
    "Address the imbalance in the data by exploring technique mentioned in the following article\n",
    "\n",
    "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
       "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
       "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
       "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
       "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
       "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
       "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
       "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
       "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
       "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
       "       'sensor_51', 'machine_status', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract NOT NORMAL for bootstrapping \n",
    "NotNormal = df2[df2['machine_status']==0]\n",
    "# Extract NORMAL \n",
    "Normal = df2[df2['machine_status']==1]\n",
    "# NOT NORMAL\n",
    "NotNormal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap sample\n",
    "# Resample observations that is 40% of the original data set\n",
    "bs_sample=NotNormal.sample(round(0.4*df2.shape[0]), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    87808\n",
       "Name: machine_status, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check and verify counts\n",
    "bs_sample['machine_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    205067\n",
       "0     87808\n",
       "Name: machine_status, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index\n",
    "bs_sample.reset_index(drop=True, inplace=True)\n",
    "# Combine bootstrap of NOT NORMAL and NORMAL into single dataframe\n",
    "dfFinal = pd.concat([bs_sample, Normal]).reset_index(drop=True)\n",
    "dfFinal['machine_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4: Feature scaling\n",
    "The continuous variables in our dataset are at varying scales. For instance if you refer back to the histograms above you can see that the variable “sensor_00” ranges from 0 to 2.54, whilst “sensor_31” ranges from 24 to 1800. This poses a problem for many popular machine learning algorithms which often use Euclidian distance between data points to make the final predictions. Standardising the scale for all continuous variables can often result in an increase in performance of machine learning models. As the values don't follow normal distribution, I will use MinMaxScaler as opposed to StandardScaler.\n",
    "\n",
    "Useful article about the topic: https://medium.com/vickdata/four-feature-types-and-how-to-transform-them-for-machine-learning-8693e1c24e80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
       "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
       "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
       "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
       "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
       "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
       "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
       "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
       "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
       "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
       "       'sensor_51', 'machine_status', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "cols = ['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
    "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
    "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
    "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
    "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
    "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
    "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
    "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
    "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
    "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
    "       'sensor_51']\n",
    "x = dfFinal[cols]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "scaled = pd.DataFrame(x_scaled, columns=cols)\n",
    "add = dfFinal.drop(cols, axis=1)\n",
    "df_scaled = pd.concat([scaled, add], axis=1)\n",
    "df_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    205067\n",
       "0     87808\n",
       "Name: machine_status, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled['machine_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5:  Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the data ready\n",
    "features=['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
    "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
    "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
    "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
    "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
    "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
    "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
    "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
    "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
    "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
    "       'sensor_51']\n",
    "x = df_scaled.loc[:, features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(x)\n",
    "p_components = range(pca.n_components_)\n",
    "_ = plt.figure(figsize=(12,5))\n",
    "_ = plt.bar(p_components, pca.explained_variance_)\n",
    "_ = plt.xlabel('PCA feature')\n",
    "_ = plt.ylabel('Variance')\n",
    "_ = plt.xticks(p_components)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['pca1', 'pca2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>machine_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.016670</td>\n",
       "      <td>0.822949</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.055637</td>\n",
       "      <td>0.316183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.013143</td>\n",
       "      <td>0.824807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.066618</td>\n",
       "      <td>0.217389</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.001901</td>\n",
       "      <td>0.270442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pca1      pca2  machine_status\n",
       "0 -1.016670  0.822949               0\n",
       "1 -1.055637  0.316183               0\n",
       "2 -1.013143  0.824807               0\n",
       "3 -1.066618  0.217389               0\n",
       "4 -1.001901  0.270442               0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = pd.concat([principalDf, df_scaled[['machine_status']]], axis = 1)\n",
    "finalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    205067\n",
       "0     87808\n",
       "Name: machine_status, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf['machine_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3852764 , 0.29541219])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare x and y \n",
    "features = ['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
    "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
    "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
    "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
    "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
    "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
    "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
    "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
    "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
    "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
    "       'sensor_51']\n",
    "x=df2[features]\n",
    "y=df2['machine_status']\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1: KNN - Unscaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996355740467984"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train KNN on training set with k=6\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "knn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1]\n",
      " [ 4350 61507]]\n"
     ]
    }
   ],
   "source": [
    "# Look at the prediction outcome\n",
    "unique_elements, counts_elements = np.unique(y_pred, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1]\n",
      " [ 4336 61521]]\n"
     ]
    }
   ],
   "source": [
    "# Look at the labels from the actual test set\n",
    "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: KNN - Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Prepare data\n",
    "x = df2[['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
    "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
    "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
    "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
    "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
    "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
    "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
    "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
    "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
    "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
    "       'sensor_51']]\n",
    "y = df2['machine_status']\n",
    "# Setup the pipeline\n",
    "steps = [('scaler', StandardScaler()),('knn', KNeighborsClassifier())]\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "# Setup the hyperparameter grid\n",
    "param_grid = {'knn__n_neighbors': np.arange(1,15)}\n",
    "# Create train and test sets (hold-out sets)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)\n",
    "# Instantiate the GridSearchCV\n",
    "knn_cv = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "# Fit to the training set\n",
    "knn_cv.fit(x_train, y_train)\n",
    "# Predict the labels of the test set\n",
    "y_pred = knn_cv.predict(x_test)\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(knn_cv.best_score_))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(knn_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(y_pred, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3: KNN - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-d4808af5ff3d>\", line 23, in <module>\n",
      "    knn_cv.fit(x_train, y_train)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 736, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 1188, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 715, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1032, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 206, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 570, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 560, in _fit_and_score\n",
      "    test_scores = _score(estimator, X_test, y_test, scorer)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 607, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 90, in __call__\n",
      "    score = scorer(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 372, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 119, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 611, in score\n",
      "    return self.steps[-1][-1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 499, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 189, in predict\n",
      "    mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\", line 572, in mode\n",
      "    modes[ind], counts[ind] = _mode1D(a_view[ind])\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\", line 559, in _mode1D\n",
      "    vals, cnts = np.unique(a, return_counts=True)\n",
      "  File \"<__array_function__ internals>\", line 6, in unique\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 263, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 303, in _unique1d\n",
      "    ar = np.asanyarray(ar).flatten()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Bauyrjan.Jyenis\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Import libaries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import warnings\n",
    "# Prepare x and y - let's start with four features (sensors 04, 28, 31, 36) as per what was discovered from EDA\n",
    "x=finalDf[['pca1', 'pca2']]\n",
    "y=finalDf['machine_status']\n",
    "# Set up steps\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "# Setup the hyperparameter grid\n",
    "param_grid = {'knn__n_neighbors': np.arange(1,50)}\n",
    "# Create train and test sets (hold-out sets)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)\n",
    "# Instantiate the GridSearchCV\n",
    "knn_cv = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "# Fit to the training set\n",
    "knn_cv.fit(x_train, y_train)\n",
    "# Predict the labels of the test set\n",
    "y_pred = knn_cv.predict(x_test)\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(knn_cv.best_score_))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(knn_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4: Logistic Regression - Unscaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Prepare data\n",
    "x = df2[['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
    "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
    "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
    "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
    "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
    "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
    "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
    "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
    "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
    "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
    "       'sensor_51']]\n",
    "y = df2['machine_status']\n",
    "# Setup the pipeline\n",
    "steps = [('logreg', LogisticRegression())]\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "# Setup the hyperparameter grid\n",
    "param_grid = {'logreg__C': np.logspace(-5, 8, 15)}\n",
    "# Create train and test sets (hold-out sets)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)\n",
    "# Instantiate the GridSearchCV\n",
    "logreg_cv = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "# Fit to the training set\n",
    "logreg_cv.fit(x_train, y_train)\n",
    "# Predict the labels of the test set\n",
    "y_pred = logreg_cv.predict(x_test)\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(logreg_cv.score(x_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(logreg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg_cv.predict_proba(x_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5: Logistic Regression - Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Prepare data\n",
    "x = df2[['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
    "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
    "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
    "       'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20',\n",
    "       'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25',\n",
    "       'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30',\n",
    "       'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35',\n",
    "       'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40',\n",
    "       'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45',\n",
    "       'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50',\n",
    "       'sensor_51']]\n",
    "y = df2['machine_status']\n",
    "# Setup the pipeline\n",
    "steps = [('scaler', StandardScaler()), ('logreg', LogisticRegression())]\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "# Setup the hyperparameter grid\n",
    "param_grid = {'logreg__C': np.logspace(-5, 8, 15)}\n",
    "# Create train and test sets (hold-out sets)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)\n",
    "# Instantiate the GridSearchCV\n",
    "logreg_cv = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "# Fit to the training set\n",
    "logreg_cv.fit(x_train, y_train)\n",
    "# Predict the labels of the test set\n",
    "y_pred = logreg_cv.predict(x_test)\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(logreg_cv.score(x_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(logreg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6: Logistic Regression - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=finalDf[['pca1', 'pca2']]\n",
    "y=finalDf['machine_status']\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=21, stratify=y)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred = logreg.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(x_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Declare a variable called gini_model and use tree.DecisionTreeClassifier. \n",
    "gini_model = tree.DecisionTreeClassifier(criterion='gini', random_state=46)\n",
    "# Set up pipeline steps\n",
    "steps = [('gini', gini_model)]\n",
    "pipeline = Pipeline(steps)\n",
    "# Declare max_depth parameter values\n",
    "param_grid = {'gini__max_depth': [0, 3, 5, 7, 9]}\n",
    "# Instantiate GridSearchCV\n",
    "gini_cv = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "# Call fit() on entr_model\n",
    "gini_cv.fit(x_train, y_train)\n",
    "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
    "y_pred = gini_cv.predict(x_test)\n",
    "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
    "pd.Series(y_pred)\n",
    "# Check out entr_model\n",
    "gini_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the cross-validation gini model\n",
    "from sklearn import metrics\n",
    "print(\"Gini impurity model - Cross Validation\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = 1))\n",
    "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = 0))\n",
    "print(\"Tuned Model Parameters: {}\".format(gini_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Instantiate Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=46)\n",
    "# Fit the model to the training set\n",
    "rf.fit(x_train, y_train)\n",
    "# Predict with the model\n",
    "y_pred = rf.predict(x_test)\n",
    "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
    "y_pred = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the Random Forest\n",
    "from sklearn import metrics\n",
    "print(\"Random Forest\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = 1))\n",
    "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
